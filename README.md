System Architecture
The architecture of the pipeline is five-staged process that is efficient and reliable. Kafka consumes data in JSON format that has rapid lossless transfer. Hadoop HDFS is fault-preventive with triple copies of data storage. Spark works with data in batches and cleans and aggregates it into formats it can use. Processed data in NiFi are provided as CSV files to other downstream ML platforms. The pipeline is orchestrated through the directed acyclic graphs (DAGs) of airflow in order to schedule the tasks every quarter. The components work as a microservice that is controlled by Docker, which contributes to the higher level of modularity and reproducibility across environments.

Implementation Details
The practice uses the adoption of Docker containers per microservice, which is isolated and reproducible. Kafka consumes Iris dataset rows and converts it to JSON before ingestion. Hadoop HDFS saves data based on the commands such as hdfs dfs -put, which is replicated three times as a redundancy measure. Spark reads and writes DataFrames in parallel computing patterns, such as the average of the petals width by the flower type. Data processed through NiFi was converted into CSV as data suitable to use in machine learning. DAGs used by Airflow handle concurrent tasks, and therefore the execution is sequential. All scripts and Docker Compose files are available in the GitHub repository
